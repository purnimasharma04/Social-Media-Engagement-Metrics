# -*- coding: utf-8 -*-
"""MediaEDA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WkSCBh_YqO-AuyNT1NqjrR4Y9HnNhJuO

#Purnima Rangavajjula
 #AM.EN.U4CSE21046
 #S5 CSE A
"""

'''installing faker package'''
!pip install faker

'''Python packages'''
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from faker import Faker
import random
from datetime import datetime, timedelta

np.random.seed(42)
# Number of records
num_records = 2000
# Generate synthetic data for Social Media User Engagement Metrics
data = {
    'user_id': np.arange(1, num_records + 1),
    'post_type': np.random.choice(['text', 'image', 'video'], num_records),
    'post_length': np.random.randint(50, 300, num_records),
    'likes': np.random.randint(0, 500, num_records),
    'comments': np.random.randint(0, 100, num_records),
    'shares': np.random.randint(0, 50, num_records),
    'engagement_rate': np.random.uniform(0.5, 5.0, num_records),
    'user_followers': np.random.randint(100, 10000, num_records),
    'post_category': np.random.choice(['sports', 'technology', 'fashion', 'food'], num_records),
    'post_hour': np.random.randint(0, 24, num_records),
    'is_weekend': np.random.choice([0, 1], num_records),
    'user_verified': np.random.choice([0, 1], num_records),
    'spam_flag': np.random.choice([0, 1], num_records)
}
# Create DataFrame
df = pd.DataFrame(data)
# Introduce NaN values for some records
nan_indices = np.random.choice(df.index, size=int(0.05 * len(df)), replace=False)
df.loc[nan_indices, 'post_length'] = np.nan
df.loc[nan_indices, 'engagement_rate'] = np.nan
# Display the first few rows of the generated dataset
print(df.head())
# Save the dataset to a CSV file
df.to_csv('social_media_engagement_metrics_dataset.csv', index=False)

excel_file_path = '/content/mediarates.xlsx'
# Write the DataFrame to an Excel file
df.to_excel(excel_file_path, index=False)
# Print a confirmation message
print(f'Dataset has been saved to {excel_file_path}')

from google.colab import files
files.download(excel_file_path)

from google.colab import drive
drive.mount('/content/drive')

df=pd.read_excel('/content/mediarates.xlsx')

df.shape

df.size

df.describe()

df.head(10)

df.tail(10)

df.describe(percentiles=[0.3,0.5,0.7])

df.describe(include=[int])

df.describe().T

df.info()

df.shape[0]

df.shape[1]

df.columns

df_copy = df.copy()
# Display the first few rows of the copied dataset
print(df_copy.head())

df_copy.isnull().head(7)

df_copy.isnull().sum()

df_copy.isnull().sum().sum()

df['comments']

df[df.index==1]

df_copy.loc[1]

df_copy.iloc[1]

df_copy.loc[100:110]

df[df.comments==47]

df.mean()

df['spam_flag'].value_counts()

df.isnull().sum()

df.notna().sum()

df.min()

df.max()

df.mean()

df.mode()

df.median()

df.columns[df.isnull().sum()>=1]

df.dtypes

df.iloc[:,[1,2,3,5]]

"""-------Matplotlib--------"""

'''Histogram'''
plt.figure(figsize=(8, 4))
# Histogram for 'likes'
plt.subplot(1, 2, 1)
plt.hist(df['likes'], bins=30, color='skyblue', edgecolor='black')
plt.title('Distribution of Likes')
plt.xlabel('Number of Likes')
plt.ylabel('Frequency')
# Histogram for 'engagement_rate'
plt.subplot(1, 2, 2)
plt.hist(df['engagement_rate'].dropna(), bins=30, color='lightcoral', edgecolor='black')
plt.title('Distribution of Engagement Rate')
plt.xlabel('Engagement Rate')
plt.ylabel('Frequency')
plt.tight_layout()
plt.show()

# Bar chart for post_type
post_type_counts = df['post_type'].value_counts()
# Plotting
plt.figure(figsize=(8, 4))
post_type_counts.plot(kind='bar', color='lightgreen')
plt.title('Count of Each Post Type')
plt.xlabel('Post Type')
plt.ylabel('Count')
plt.show()

'''Line plot'''
plt.figure(figsize=(8, 4))
plt.plot(df['post_length'].iloc[:100], df['engagement_rate'].iloc[:100], marker='o', linestyle='-', color='orange')
# Adding labels and title
plt.xlabel('Post Length')
plt.ylabel('Engagement Rate')
plt.title('Engagement Rate vs Post Length')
# Display the plot
plt.grid(True)
plt.show()

# Scatter plot
plt.figure(figsize=(8, 4))
# Scatter plot with orange color
plt.scatter(df['user_followers'], df['engagement_rate'], color='yellow', alpha=0.6)
# Set plot labels and title
plt.title('Scatter Plot of User Followers vs Engagement Rate')
plt.xlabel('User Followers')
plt.ylabel('Engagement Rate')
# Show the plot
plt.show()

'''Box plot'''
plt.figure(figsize=(8, 4))
df.boxplot(column=['post_length', 'engagement_rate','post_hour'], color='blue')
plt.title('Box Plot of Post Length and Engagement Rate')
plt.ylabel('Values')
plt.show()

'''Box plot'''
# Set a pink color palette for the box plot
sns.set_palette("pastel")
# Create a figure and axis
fig, ax = plt.subplots(figsize=(8, 4))
# Box plot for 'post_length' and 'engagement_rate' with pink color
sns.boxplot(data=df[['post_length', 'engagement_rate']], color='pink', ax=ax)
# Highlight outliers with a different color
sns.stripplot(data=df[['post_length', 'engagement_rate']], color='red', size=3, jitter=True, ax=ax)
# Set plot labels and title
ax.set(xlabel='Metrics', ylabel='Values', title='Box Plot of Social Media Metrics with Outliers')
# Display the plot
plt.show()

'''pie chart'''
# Set a dark color palette
dark_colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']
# Count the number of posts in each category
category_counts = df['post_category'].value_counts()
# Plotting the pie chart
plt.figure(figsize=(8, 4))
plt.pie(category_counts, labels=category_counts.index, autopct='%1.1f%%', colors=dark_colors, startangle=90)
plt.title('Distribution of Posts by Category')
plt.show()

# Count of missing values in each column
missing_count = df.isnull().sum()
# Percentage of missing values in each column
missing_percentage = (missing_count / len(df)) * 100
# Create a DataFrame to display the results
missing_info = pd.DataFrame({
    'Missing Count': missing_count,
    'Missing Percentage': missing_percentage
})
# Display the missing values information
print(missing_info)

'''Missing value Imputation'''
import missingno as msno
# Impute missing values (using mean in this example)
df['post_length'].fillna(df['post_length'].mean(), inplace=True)
df['engagement_rate'].fillna(df['engagement_rate'].mean(), inplace=True)
# Visualize again after imputation
msno.matrix(df)
plt.show()
msno.bar(df)
plt.show()

most_frequent_shares = df['shares'].mode().iloc[0]
# Replace missing values in the 'shares' column with the most frequent value
df['shares'].fillna(most_frequent_shares, inplace=True)
# Display the first few rows of the updated dataset
print(df.head())
msno.bar(df)

'''most common class'''
most_common_post_length = df['post_length'].mode()[0]
df['post_length'].fillna(most_common_post_length, inplace=True)
msno.bar(df)

from sklearn.impute import KNNImputer
# Copy the DataFrame to preserve the original data
df_imputed = df.copy()
# Initialize KNNImputer
imputer = KNNImputer(n_neighbors=5)
# Impute missing values in the 'engagement_rate' column
imputer_columns = ['post_length', 'engagement_rate']  # Columns to impute
df_imputed[imputer_columns] = imputer.fit_transform(df_imputed[imputer_columns])
# Display the first few rows of the imputed dataset
print(df_imputed.head())
msno.bar(df_imputed)

from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
# Creating an IterativeImputer instance
imputer = IterativeImputer(random_state=0)
# Columns to consider for imputation (excluding user_id which is not relevant for imputation)
columns_for_imputation = ['post_length', 'likes', 'comments', 'shares', 'user_followers', 'post_hour', 'is_weekend', 'user_verified', 'spam_flag']
# Fit the imputer on the columns and transform the dataset
df_imputed = df.copy()  # Create a copy of the DataFrame
df_imputed[columns_for_imputation] = imputer.fit_transform(df_imputed[columns_for_imputation])
# Display the first few rows of the dataset with imputed values
print(df_imputed.head())
msno.bar(df_imputed)

df_cleaned = df.dropna(subset=['post_length', 'engagement_rate'])
# Display the first few rows of the cleaned dataset
print(df_cleaned.head())
msno.bar(df_cleaned)

df.isnull().sum()

nan_indices_categorical = np.random.choice(df.index, size=int(0.1 * len(df)), replace=False)
df.loc[nan_indices_categorical, 'post_category'] = np.nan
# Introduce NaN values for some records in post_length and engagement_rate columns
nan_indices_numeric = np.random.choice(df.index, size=int(0.05 * len(df)), replace=False)
df.loc[nan_indices_numeric, 'post_length'] = np.nan
df.loc[nan_indices_numeric, 'engagement_rate'] = np.nan
print(df)

df.isnull()

df.isnull().sum()

most_common_post_type = df['post_length'].mode()[0]
df['post_length'].fillna(most_common_post_length, inplace=True)
# Display the first few rows of the dataset after imputation
print(df.head())

df.isnull().sum()

'''Unknown class'''
df['post_category'].fillna("Unknown", inplace=True)
# Display the first few rows of the dataset with imputed values
print(df.head())

df.isnull().sum()

missing_values = df['engagement_rate'].isnull()
# Calculate the mode of the 'engagement_rate' column
mode_engagement_rate = df['engagement_rate'].mode()[0]
# Fill missing values with the mode
df.loc[missing_values, 'engagement_rate'] = mode_engagement_rate
# Display the first few rows of the DataFrame after imputation
print(df.head())

df.isnull().sum()

"""-----Remove Noise from data------"""

df_cleaned = df.dropna()
# OR
# Impute missing values with the mean
df_cleaned = df.fillna(df.mean())

Q1 = df_cleaned.quantile(0.25)
Q3 = df_cleaned.quantile(0.75)
IQR = Q3 - Q1
# Remove outliers
df_cleaned = df_cleaned[~((df_cleaned < (Q1 - 1.5 * IQR)) | (df_cleaned > (Q3 + 1.5 * IQR))).any(axis=1)]

from sklearn.preprocessing import StandardScaler
# Standardize numerical features
scaler = StandardScaler()
df_cleaned[['post_length', 'likes', 'comments', 'shares', 'engagement_rate', 'user_followers']] = scaler.fit_transform(df_cleaned[['post_length', 'likes', 'comments', 'shares', 'engagement_rate', 'user_followers']])

print(df_cleaned)

# One-hot encode categorical variables
df_cleaned = pd.get_dummies(df_cleaned, columns=['post_type', 'post_category'])

df.shape

df.min

df.max

df.info()

df.describe()

df.isnull().sum()

# Columns for histogram analysis
columns_for_histogram = ['likes', 'comments', 'user_followers']
# Plotting histograms for selected columns
for column in columns_for_histogram:
    plt.figure(figsize=(8, 6))
    plt.hist(df[column].dropna(), bins=30, color='purple')  # Drop NaN values before plotting
    plt.title(f'Histogram of {column}')
    plt.xlabel(column)
    plt.ylabel('Frequency')
    plt.grid(True)
    plt.show()

"""Binning"""

# Define the number of bins
num_bins = 5
# Perform binning by mean for 'post_length'
df['post_length_bin'] = pd.cut(df['post_length'], bins=num_bins, labels=False)
post_length_means = df.groupby('post_length_bin')['post_length'].mean()
df['post_length_binned_mean'] = df['post_length_bin'].map(post_length_means)
# Perform binning by mean for 'engagement_rate'
df['engagement_rate_bin'] = pd.cut(df['engagement_rate'], bins=num_bins, labels=False)
engagement_rate_means = df.groupby('engagement_rate_bin')['engagement_rate'].mean()
df['engagement_rate_binned_mean'] = df['engagement_rate_bin'].map(engagement_rate_means)
# Display the modified DataFrame
print(df[['user_id', 'post_length', 'post_length_binned_mean', 'engagement_rate', 'engagement_rate_binned_mean']].head())

# Binning by mode on the 'post_length' column
mode_bins = pd.cut(df['post_length'], bins=3, labels=['Short', 'Medium', 'Long'], include_lowest=True, duplicates='drop')
# Create a new column 'post_length_bin' in the DataFrame
df['post_length_bin'] = mode_bins
# Display the modified DataFrame
print("\nDataFrame with Binned 'post_length':")
print(df[['post_length', 'post_length_bin']].head())

# Specify bin boundaries for post_length
bin_boundaries = [50, 100, 150, 200, 250, 300]
# Perform binning on the 'post_length' column
df['post_length_bins'] = pd.cut(df['post_length'], bins=bin_boundaries, labels=False, right=False)
# Display the first few rows of the DataFrame with the new bin column
print(df[['post_length', 'post_length_bins']].head())

"""Data Visulisations"""

df=pd.read_excel('/content/mediarates.xlsx')

df.shape

df.info()

df.describe()

df.size

df.mean()

df.mode()

df.median()

df.min()

df.max()

df.count()

df.isnull().sum()

most_common_post_type = df['post_length'].mode()[0]
df['post_length'].fillna(most_common_post_length, inplace=True)
# Display the first few rows of the dataset after imputation
print(df.head())

df.isnull().sum()

most_common_engagement_rate = df['engagement_rate'].mode()[0]
df['engagement_rate'].fillna(most_common_post_length, inplace=True)
# Display the first few rows of the dataset after imputation
print(df.head())

df.isnull().sum()

# Create a distribution plot for 'post_length' using Seaborn
plt.figure(figsize=(8, 4))
sns.histplot(data=df, x='post_length', kde=True, bins=30, color='gold')
# Add title and labels
plt.title('Distribution of Post Length')
plt.xlabel('Post Length')
plt.ylabel('Frequency')
# Show the plot
plt.show()

df.dtypes

# Create a histogram using Seaborn for the 'likes' column
plt.figure(figsize=(8, 4))
sns.histplot(data=df, x='likes', bins=30, kde=True, color='violet')
plt.title('Distribution of Likes in Social Media Posts')
plt.xlabel('Number of Likes')
plt.ylabel('Frequency')
plt.show()

# Plot histograms for all features using Seaborn
for column in df.columns:
    if df[column].dtype in [np.int64, np.float64]:  # Check if the column contains numerical data
        plt.figure(figsize=(8, 4))
        sns.histplot(df[column].dropna(), kde=True, bins=30, color='silver')
        plt.title(f'Histogram of {column}')
        plt.xlabel(column)
        plt.ylabel('Frequency')
        plt.show()

# Display the correlation matrix
correlation_matrix = df.corr()
print(correlation_matrix)

corr_matrix = df.corr()
# Plot correlation heatmap
plt.figure(figsize=(8, 4))
sns.heatmap(corr_matrix, annot=True, cmap='viridis', fmt='.2f', linewidths=.5)
plt.title('Correlation Heatmap')
plt.show()
# Plot histograms for selected columns
selected_columns = ['post_length', 'likes', 'comments', 'shares', 'engagement_rate', 'user_followers']
df[selected_columns].hist(bins=20, figsize=(10, 12))
plt.suptitle('Histograms for Selected Columns', y=0.92)
plt.show()

# Create subplots
fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15,5))
# Plot 1: post_length distribution
axes[0].hist(df['post_length'].dropna(), bins=20, color='skyblue', edgecolor='black')
axes[0].set_title('Post Length Distribution')
axes[0].set_xlabel('Post Length')
axes[0].set_ylabel('Frequency')
# Plot 2: likes vs engagement_rate
axes[1].scatter(df['likes'], df['engagement_rate'], alpha=0.5, color='coral')
axes[1].set_title('Likes vs Engagement Rate')
axes[1].set_xlabel('Likes')
axes[1].set_ylabel('Engagement Rate')
# Plot 3: Boxplot of engagement_rate by post_type
df.boxplot(column='engagement_rate', by='post_type', ax=axes[2], grid=False, patch_artist=True)
axes[2].set_title('Engagement Rate by Post Type')
axes[2].set_xlabel('Post Type')
axes[2].set_ylabel('Engagement Rate')
plt.tight_layout()
plt.show()

sns.set(style="whitegrid")
# Create a box plot with outliers
plt.figure(figsize=(10, 4))
sns.boxplot(x='post_length', data=df, showfliers=True)
# Add labels and title
plt.title('Box Plot of Post Length with Outliers')
plt.xlabel('Post Length')
# Show the plot
plt.show()

# Box plot for 'likes', 'comments', and 'shares'
plt.figure(figsize=(12, 8))
# Create a subplot for each metric
plt.subplot(3, 1, 1)
sns.boxplot(x='post_type', y='likes', data=df)
plt.title('Box Plot for Likes')
plt.subplot(3, 1, 2)
sns.boxplot(x='post_type', y='comments', data=df)
plt.title('Box Plot for Comments')
plt.subplot(3, 1, 3)
sns.boxplot(x='post_type', y='shares', data=df)
plt.title('Box Plot for Shares')
plt.tight_layout()
plt.show()

"""EDA"""

df=pd.read_excel('/content/mediarates.xlsx')

df.head()

df.shape

df.info()

df.dtypes

df.columns

import matplotlib.pyplot as plt
import seaborn as sns
# Set the style for seaborn
sns.set(style="whitegrid")
# Create subplots
fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(8, 15))
fig.suptitle('Social Media User Engagement Metrics', fontsize=16)
# Subplot 1: Likes vs. Post Type
sns.barplot(x='post_type', y='likes', data=df, ax=axes[0], ci=None, palette='viridis')
axes[0].set_title('Likes vs. Post Type')
# Subplot 2: Comments vs. Post Category
sns.barplot(x='post_category', y='comments', data=df, ax=axes[1], ci=None, palette='muted')
axes[1].set_title('Comments vs. Post Category')
# Subplot 3: Shares vs. Post Hour
sns.barplot(x='post_hour', y='shares', data=df, ax=axes[2], ci=None, palette='Set2')
axes[2].set_title('Shares vs. Post Hour')
# Adjust layout
plt.tight_layout(rect=[0, 0, 1, 0.96])
# Show the plot
plt.show()

# Select numerical columns for distribution analysis
numerical_columns = ['post_length', 'likes', 'comments', 'shares', 'engagement_rate', 'user_followers']
# Plot histograms for numerical features
for column in numerical_columns:
    plt.figure(figsize=(8, 5))
    sns.histplot(df[column].dropna(), kde=True, bins=30, color='red')
    plt.title(f'Distribution of {column}')
    plt.xlabel(column)
    plt.ylabel('Frequency')
    plt.show()

from scipy.stats import skew
# Assuming you have already generated and displayed the DataFrame df
# Selecting numerical columns
numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns
# Calculate skewness for each numerical column
skewness_values = df[numerical_columns].apply(lambda x: skew(x.dropna()))
# Creating a DataFrame to display skewness values
skewness_df = pd.DataFrame({'Feature': numerical_columns, 'Skewness': skewness_values})
# Displaying the skewness values
print(skewness_df)

# Handle Missing Values
df['post_length'].fillna(df['post_length'].mean(), inplace=True)
df['engagement_rate'].fillna(df['engagement_rate'].mean(), inplace=True)
# Convert Data Types
df[['is_weekend', 'user_verified', 'spam_flag']] = df[['is_weekend', 'user_verified', 'spam_flag']].astype(bool)
# Perform Categorical Encoding
df = pd.get_dummies(df, columns=['post_type', 'post_category'], drop_first=True)
# Display the first few rows of the cleaned dataset
print(df.head())

df.isnull().sum()

numerical_cols = ['post_length', 'likes', 'comments', 'shares', 'engagement_rate', 'user_followers']
# Drop NaN values for pairplot
df_pairplot = df[numerical_cols].dropna()
# Create pairplot
sns.pairplot(df_pairplot)
plt.show()

sns.set(style="whitegrid")
# Define the columns for which you want to create box plots
numeric_columns = ['post_length', 'likes', 'comments', 'shares', 'engagement_rate', 'user_followers']
# Create box plots
plt.figure(figsize=(10, 8))
for i, column in enumerate(numeric_columns, 1):
    plt.subplot(2, 3, i)
    sns.boxplot(x=df[column])
    plt.title(f'Box Plot of {column}')
plt.tight_layout()
plt.show()

from sklearn.neighbors import LocalOutlierFactor
# Your existing code to generate the dataset
# ...
# Specify the columns for LOF analysis
lof_columns = ['post_length', 'likes', 'comments', 'shares', 'engagement_rate', 'user_followers']
# Create a copy of the DataFrame to keep the original intact
df_before_outliers = df.copy()
# Extract the relevant columns for LOF analysis
data_for_lof = df[lof_columns].fillna(0)  # Fill NaN values with 0 for LOF
# Fit LOF model
lof = LocalOutlierFactor(n_neighbors=20, contamination=0.05)
outlier_scores = lof.fit_predict(data_for_lof)
# Identify and remove outliers
outliers_mask = outlier_scores != -1
df = df.loc[outliers_mask].reset_index(drop=True)
# Display the shape before and after removing outliers
print("Shape before removing outliers:", df_before_outliers.shape)
print("Shape after removing outliers:", df.shape)

# Set the style for the plots
sns.set(style="whitegrid")
# Define the columns for analysis
continuous_columns = ['post_length', 'likes', 'comments', 'shares', 'engagement_rate', 'user_followers']
# Plot histograms for each continuous column
for column in continuous_columns:
    plt.figure(figsize=(8, 6))
    sns.histplot(df[column].dropna(), kde=True, color='pink', bins=30)
    plt.title(f'Distribution of {column}')
    plt.xlabel(column)
    plt.ylabel('Frequency')
    plt.show()

# Set style for better visualization
sns.set(style="whitegrid")
# Define the discrete variables for analysis
discrete_vars = [ 'post_hour', 'is_weekend', 'user_verified', 'spam_flag']
# Plot histograms for each discrete variable
for var in discrete_vars:
    plt.figure(figsize=(10, 5))
    sns.countplot(x=var, data=df, palette="viridis")
    plt.title(f'Distribution of {var}')
    plt.show()

df.isnull().sum()

# Calculate the correlation matrix
correlation_matrix = df.corr()
# Plot the heatmap
plt.figure(figsize=(12, 10))
print(correlation_matrix)
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Correlation Matrix of Social Media User Engagement Metrics')
plt.show()

# Display the correlation matrix
print("Correlation Matrix:")
print(correlation_matrix)
# Extract strongly correlated pairs
strongly_correlated_pairs = correlation_matrix.unstack().sort_values(ascending=False).drop_duplicates()
strongly_correlated_pairs = strongly_correlated_pairs[strongly_correlated_pairs != 1]

# Display strongly correlated pairs
print("\nStrongly Correlated Feature Pairs:")
print(strongly_correlated_pairs)

# Select features for pairplot
selected_features = ['post_length', 'likes', 'comments', 'shares', 'engagement_rate', 'user_followers']

# Create a pairplot
sns.pairplot(df[selected_features], diag_kind='kde', markers='o', hue='post_length', palette='viridis')

# Show the plot
plt.show()